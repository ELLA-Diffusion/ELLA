{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment \n",
    "\n",
    "make sure to download the model from: `https://huggingface.co/QQGYLab/ELLA/blob/main/ella-sd1.5-tsc-t5xl.safetensors`  \n",
    "and save it thus `/ella/models/ella_path/ella-sd1.5-tsc-t5xl.safetensors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Dependencies and Define Classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors \n",
    "!python ella_gen.py \n",
    "!pip install torch \n",
    "!pip install diffusers \n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install accelerate \n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Optional, Union\n",
    "import os\n",
    "import sys\n",
    "import safetensors.torch\n",
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model import ELLA, T5TextEmbedder\n",
    "\n",
    "\n",
    "class ELLAProxyUNet(torch.nn.Module):\n",
    "    def __init__(self, ella, unet):\n",
    "        super().__init__()\n",
    "        self.ella = ella\n",
    "        self.unet = unet\n",
    "        self.config = unet.config\n",
    "        self.dtype = unet.dtype\n",
    "        self.device = unet.device\n",
    "        self.flexible_max_length_workaround = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        timestep_cond: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_kwargs: Optional[dict[str, Any]] = None,\n",
    "        added_cond_kwargs: Optional[dict[str, torch.Tensor]] = None,\n",
    "        down_block_additional_residuals: Optional[tuple[torch.Tensor]] = None,\n",
    "        mid_block_additional_residual: Optional[torch.Tensor] = None,\n",
    "        down_intrablock_additional_residuals: Optional[tuple[torch.Tensor]] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        if self.flexible_max_length_workaround is not None:\n",
    "            time_aware_encoder_hidden_state_list = []\n",
    "            for i, max_length in enumerate(self.flexible_max_length_workaround):\n",
    "                time_aware_encoder_hidden_state_list.append(\n",
    "                    self.ella(encoder_hidden_states[i : i + 1, :max_length], timestep)\n",
    "                )\n",
    "            time_aware_encoder_hidden_states = torch.cat(\n",
    "                time_aware_encoder_hidden_state_list, dim=0\n",
    "            )\n",
    "        else:\n",
    "            time_aware_encoder_hidden_states = self.ella(\n",
    "                encoder_hidden_states, timestep\n",
    "            )\n",
    "\n",
    "        return self.unet(\n",
    "            sample=sample,\n",
    "            timestep=timestep,\n",
    "            encoder_hidden_states=time_aware_encoder_hidden_states,\n",
    "            class_labels=class_labels,\n",
    "            timestep_cond=timestep_cond,\n",
    "            attention_mask=attention_mask,\n",
    "            cross_attention_kwargs=cross_attention_kwargs,\n",
    "            added_cond_kwargs=added_cond_kwargs,\n",
    "            down_block_additional_residuals=down_block_additional_residuals,\n",
    "            mid_block_additional_residual=mid_block_additional_residual,\n",
    "            down_intrablock_additional_residuals=down_intrablock_additional_residuals,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            return_dict=return_dict,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_image_with_flexible_max_length(\n",
    "    pipe, t5_encoder, prompt, negative_prompt=None, fixed_negative=False, output_type=\"pt\", **pipe_kwargs\n",
    "):\n",
    "    device = pipe.device\n",
    "    dtype = pipe.dtype\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    prompt_embeds = t5_encoder(prompt, max_length=None).to(device, dtype)\n",
    "    \n",
    "    # Use the provided negative prompt if any, otherwise use empty strings\n",
    "    negative_prompt = [negative_prompt] * batch_size if negative_prompt else [\"\"] * batch_size\n",
    "    negative_prompt_embeds = t5_encoder(\n",
    "        negative_prompt, max_length=256 if fixed_negative else None\n",
    "    ).to(device, dtype)\n",
    "\n",
    "    pipe.unet.flexible_max_length_workaround = [\n",
    "        negative_prompt_embeds.size(1)\n",
    "    ] * batch_size + [prompt_embeds.size(1)] * batch_size\n",
    "\n",
    "    max_length = max([prompt_embeds.size(1), negative_prompt_embeds.size(1)])\n",
    "    b, _, d = prompt_embeds.shape\n",
    "    prompt_embeds = torch.cat(\n",
    "        [\n",
    "            prompt_embeds,\n",
    "            torch.zeros(\n",
    "                (b, max_length - prompt_embeds.size(1), d), device=device, dtype=dtype\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    negative_prompt_embeds = torch.cat(\n",
    "        [\n",
    "            negative_prompt_embeds,\n",
    "            torch.zeros(\n",
    "                (b, max_length - negative_prompt_embeds.size(1), d),\n",
    "                device=device,\n",
    "                dtype=dtype,\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    images = pipe(\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "        **pipe_kwargs,\n",
    "        output_type=output_type,\n",
    "    ).images\n",
    "    pipe.unet.flexible_max_length_workaround = None\n",
    "    return images\n",
    "\n",
    "def load_ella(filename, device, dtype):\n",
    "    ella = ELLA()\n",
    "    safetensors.torch.load_model(ella, filename, strict=True)\n",
    "    ella.to(device, dtype=dtype)\n",
    "    return ella\n",
    "\n",
    "def load_ella_for_pipe(pipe, ella):\n",
    "    pipe.unet = ELLAProxyUNet(ella, pipe.unet)\n",
    "\n",
    "def offload_ella_for_pipe(pipe):\n",
    "    pipe.unet = pipe.unet.unet\n",
    "\n",
    "def generate_image_with_fixed_max_length(\n",
    "    pipe, t5_encoder, prompt, negative_prompt=None, output_type=\"pt\", **pipe_kwargs\n",
    "):\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "\n",
    "    prompt_embeds = t5_encoder(prompt, max_length=256).to(pipe.device, pipe.dtype)\n",
    "    \n",
    "    # Use the provided negative prompt if any, otherwise use empty strings\n",
    "    negative_prompt = [negative_prompt] * len(prompt) if negative_prompt else [\"\"] * len(prompt)\n",
    "    negative_prompt_embeds = t5_encoder(negative_prompt, max_length=256).to(\n",
    "        pipe.device, pipe.dtype\n",
    "    )\n",
    "\n",
    "    return pipe(\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "        **pipe_kwargs,\n",
    "        output_type=output_type,\n",
    "    ).images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models and set up the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  4.52it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.00it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set paths to the model and prompts file\n",
    "save_folder = \"output_images\"\n",
    "ella_path = \"../ella/models/ella_path/ella-sd1.5-tsc-t5xl.safetensors\"  # Update this path to the actual ELLA model path\n",
    "prompt_file = \"prompts.txt\"  # Ensure this file exists and is correct\n",
    "\n",
    "# Ensure save folder exists\n",
    "save_folder = Path(save_folder)\n",
    "save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if ELLA model path is correct\n",
    "if not Path(ella_path).exists():\n",
    "    raise FileNotFoundError(f\"ELLA model file not found at {ella_path}\")\n",
    "\n",
    "# Check if prompt file exists\n",
    "if not Path(prompt_file).exists():\n",
    "    raise FileNotFoundError(f\"Prompt file not found at {prompt_file}\")\n",
    "\n",
    "# Load pipeline and models\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "ella = load_ella(ella_path, pipe.device, pipe.dtype)\n",
    "t5_encoder = T5TextEmbedder().to(pipe.device, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_save_images(prompts, negative_prompt=None):\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f'Generating images for prompt: {prompt}')\n",
    "        _batch_size = 1\n",
    "        size = 512\n",
    "        seed = 1001\n",
    "        prompt_list = [prompt] * _batch_size\n",
    "\n",
    "        load_ella_for_pipe(pipe, ella)\n",
    "        image_flexible = generate_image_with_flexible_max_length(\n",
    "            pipe,\n",
    "            t5_encoder,\n",
    "            prompt_list,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=11,\n",
    "            num_inference_steps=70,\n",
    "            height=size,\n",
    "            width=size,\n",
    "            generator=[\n",
    "                torch.Generator(device=\"cuda\").manual_seed(seed + j)\n",
    "                for j in range(_batch_size)\n",
    "            ],\n",
    "        )\n",
    "        image_fixed = generate_image_with_fixed_max_length(\n",
    "            pipe,\n",
    "            t5_encoder,\n",
    "            prompt_list,\n",
    "            negative_prompt=negative_prompt,\n",
    "            guidance_scale=11,\n",
    "            num_inference_steps=70,\n",
    "            height=size,\n",
    "            width=size,\n",
    "            generator=[\n",
    "                torch.Generator(device=\"cuda\").manual_seed(seed + j)\n",
    "                for j in range(_batch_size)\n",
    "            ],\n",
    "        )\n",
    "        offload_ella_for_pipe(pipe)\n",
    "\n",
    "        image_ori = pipe(\n",
    "            prompt_list,\n",
    "            negative_prompt=[negative_prompt] * _batch_size,\n",
    "            output_type=\"pt\",\n",
    "            guidance_scale=11,\n",
    "            num_inference_steps=70,\n",
    "            height=size,\n",
    "            width=size,\n",
    "            generator=[\n",
    "                torch.Generator(device=\"cuda\").manual_seed(seed + j)\n",
    "                for j in range(_batch_size)\n",
    "            ],\n",
    "        ).images\n",
    "\n",
    "        local_save_path = save_folder / f\"{i:03d}.png\"\n",
    "        save_image(\n",
    "            torch.cat([image_ori, image_fixed, image_flexible], dim=0),\n",
    "            local_save_path,\n",
    "            nrow=3,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prompts and generate images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: Crocodile in a sweater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 14.78it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.94it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: a large, textured green crocodile lying comfortably on a patch of grass with a cute, knitted orange sweater enveloping its scaly body. Around its neck, the sweater features a whimsical pattern of blue and yellow stripes. In the background, a smooth, grey rock partially obscures the view of a small pond with lily pads floating on the surface.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.59it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.85it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: A red book and a yellow vase.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.59it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.71it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: A vivid red book with a smooth, matte cover lies next to a glossy yellow vase. The vase, with a slightly curved silhouette, stands on a dark wood table with a noticeable grain pattern. The book appears slightly worn at the edges, suggesting frequent use, while the vase holds a fresh array of multicolored wildflowers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.39it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.54it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: a racoon holding a shiny red apple over its head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.29it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.47it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: a mischievous raccoon standing on its hind legs, holding a bright red apple aloft in its furry paws. the apple shines brightly against the backdrop of a dense forest, with leaves rustling in the gentle breeze. a few scattered rocks can be seen on the ground beneath the raccoon's feet, while a gnarled tree trunk stands nearby.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.15it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.30it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: a chinese man wearing a white shirt and a checkered headscarf, holds a large falcon near his shoulder. the falcon has dark feathers with a distinctive beak. the background consists of a clear sky and a fence, suggesting an outdoor setting, possibly a desert or arid region\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 14.84it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.40it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: A close-up photo of a wombat wearing a red backpack and raising both arms in the air. Mount Rushmore is in the background\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.24it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.49it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: An oil painting of a man in a factory looking at a cat wearing a top hat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.29it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.50it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for prompt: A young woman with long brown hair, wearing a white t-shirt, holding multiple colorful shopping bags in her hands. She has a big smile on her face and her mouth is open wide, showing her teeth. Her right hand is raised in a fist, as if she is celebrating or cheering. The background is black, making the woman and her bags the focal point of the image. . A young woman with long brown hair, wearing a white t-shirt, holds multiple colorful shopping bags in her hands. She flashes a big smile on her face, her mouth open wide, showing her teeth. With her right hand raised in a fist, she celebrates or cheers. The black background makes the woman and her bags the focal point of the image. high resolution, photorealistic, golden hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:04<00:00, 15.31it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.53it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (161 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image.. a young woman with long brown hair, wearing a white t - shirt, holds multiple colorful shopping bags in her hands. she flashes a big smile on her face, her mouth open wide, showing her teeth. with her right hand raised in a fist, she celebrates or cheers. the black background makes the woman and her bags the focal point of the image. high resolution, photorealistic, golden hours']\n",
      "100%|██████████| 70/70 [00:04<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read prompts from file\n",
    "with open(prompt_file, 'r') as f:\n",
    "    prompts = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Example of a negative prompt\n",
    "negative_prompt = \"\"\"(((deformed))), blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), \n",
    "    fused fingers, messy drawing, broken legs censor, censored, censor_bar, multiple breasts, (mutated hands and fingers:1.5), (long body :1.3), \n",
    "    (mutation, poorly drawn :1.2), black-white, bad anatomy, liquid body, liquid tongue, disfigured, malformed, mutated, anatomical nonsense, \n",
    "    text font ui, error, malformed hands, long neck, blurred, lowers, low res, bad anatomy, bad proportions, bad shadow, uncoordinated body, \n",
    "    unnatural body, fused breasts, bad breasts, huge breasts, poorly drawn breasts, extra breasts, liquid breasts, heavy breasts, missing breasts, \n",
    "    huge haunch, huge thighs, huge calf, bad hands, fused hand, missing hand, disappearing arms, disappearing thigh, disappearing calf, disappearing legs, \n",
    "    fused ears, bad ears, poorly drawn ears, extra ears, liquid ears, heavy ears, missing ears, old photo, low res, black and white, black and white filter, \n",
    "    colorless, deformed teeth, bad teeth, poorly drawn teeth, extra teeth, liquid teeth, malformed teeth, missing teeth,\"\"\"\n",
    "\n",
    "# Generate and save images\n",
    "generate_and_save_images(prompts, negative_prompt=negative_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ella_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
